{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian import HamiltonianSmall, Hamiltonian\n",
    "\n",
    "import var_opt_lagrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange by Rudy\n",
    "\n",
    "See Rudy's note on optimizing the variance with Lagrange multiplier method\n",
    "\n",
    "This notebook works for both convex cost function `diagonal` and non-convex cost function `mixed`\n",
    "\n",
    "For the non-convex setting, we need access to the Hartree-Fock bitstrings. Currently, we only have that for JW encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lih_small = HamiltonianSmall('LiH', 1.5) # this encoding uses reduction techniques\n",
    "beh2_small = HamiltonianSmall('BeH2', 1.3) # this encoding uses reduction techniques\n",
    "\n",
    "# 4 qubits\n",
    "h2_jw_4 = Hamiltonian('H2_STO3g_4qubits', 'jw')\n",
    "h2_parity_4 = Hamiltonian('H2_STO3g_4qubits', 'parity')\n",
    "h2_bk_4 = Hamiltonian('H2_STO3g_4qubits', 'bk')\n",
    "\n",
    "# 8 qubits\n",
    "h2_jw = Hamiltonian('H2_6-31G_8qubits', 'jw')\n",
    "h2_parity = Hamiltonian('H2_6-31G_8qubits', 'parity')\n",
    "h2_bk = Hamiltonian('H2_6-31G_8qubits', 'bk')\n",
    "\n",
    "# 12 qubits\n",
    "lih_jw = Hamiltonian('LiH_STO3g_12qubits', 'jw')\n",
    "\n",
    "# 14 qubits\n",
    "h2o_jw = Hamiltonian('H2O_STO3g_14qubits', 'jw')\n",
    "\n",
    "beh2_jw = Hamiltonian('BeH2_STO3g_14qubits', 'jw')\n",
    "\n",
    "# 16 qubits\n",
    "nh3_jw = Hamiltonian('NH3_STO3g_16qubits', 'jw')\n",
    "\n",
    "# 20 qubits\n",
    "c2_jw = Hamiltonian('C2_STO3g_20qubits', 'jw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamiltonians = {\"lih_small\": lih_small,\n",
    "                \"beh2_small\": beh2_small,\n",
    "                \"h2_jw_4\": h2_jw_4,\n",
    "                \"h2_parity_4\": h2_parity_4,\n",
    "                \"h2_bk_4\": h2_bk_4,\n",
    "                \"h2_jw\": h2_jw,\n",
    "                \"h2_parity\": h2_parity,\n",
    "                \"h2_bk\": h2_bk,\n",
    "                \"lih_jw\": lih_jw,\n",
    "                \"h2o_jw\": h2o_jw,\n",
    "                \"beh2_jw\": beh2_jw,\n",
    "                \"nh3_jw\": nh3_jw,\n",
    "                \"c2_jw\": c2_jw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'lagrange'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example showing how to call this from `Hamiltonian.PauliRep.local_dists_optimal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "def isProbability(betas, tol=1.0e-5):\n",
    "    \"\"\"\n",
    "        Check if betas are probabilities\n",
    "    \"\"\"\n",
    "    for k,v in betas.items():\n",
    "        for _v in v:\n",
    "            if _v < 0:\n",
    "                return False\n",
    "        if np.abs(np.sum(v) - 1.0) > tol:\n",
    "            print(\"\\tErr:\", np.abs(np.sum(v) - 1.0) )\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_rand_betas(num_qubits):\n",
    "    \"\"\"\n",
    "        generate random initial points to search for betas\n",
    "    \"\"\"\n",
    "    betas = {}\n",
    "    for qubit in range(num_qubits):\n",
    "        betas[qubit] = rand(3)\n",
    "        betas[qubit] = betas[qubit]/np.sum(betas[qubit])\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def both_variances_lagrange(ham, num_trials=3, display_betas=False):\n",
    "    t0 = time.time()\n",
    "    energy, state = ham.pauli_rep.ground(multithread=True) # you can add optional keyword num_cores=15\n",
    "    t1 = time.time()\n",
    "    # diagonal objective function\n",
    "    \n",
    "    objective = 'diagonal'\n",
    "    β_optimal_diagonal = ham.pauli_rep.local_dists_optimal(objective, method)\n",
    "    if display_betas:\n",
    "        print(\"\\tbeta (diagonal):\", β_optimal_diagonal)\n",
    "    t2 = time.time()\n",
    "    var = ham.pauli_rep.variance_local(energy, state, β_optimal_diagonal)\n",
    "    t3 = time.time()\n",
    "    print(\"Diagonal variance:\", var)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # mixed objective function\n",
    "    \n",
    "    objective = 'mixed'\n",
    "    assert ham.encoding == 'jw' # we do not have HF bitstrings for other encoding at the moment.\n",
    "    bitstring_HF = ham.read_bitstring_HF()\n",
    "    \n",
    "    for trial in range(num_trials): \n",
    "        #because we do not have convexity, we perform several trials to look for optimal betas\n",
    "        print(\"Trial:\", _)\n",
    "        if trial == 0:\n",
    "            beta_init = β_optimal_diagonal\n",
    "        else:\n",
    "            beta_init = get_rand_betas(ham.pauli_rep.num_qubits)\n",
    "        \n",
    "        if trial == num_trials-1:\n",
    "            t4 = time.time()\n",
    "        \n",
    "        β_optimal_mixed = ham.pauli_rep.local_dists_optimal(objective, method, \n",
    "                                                            bitstring_HF=bitstring_HF, β_initial=beta_init)\n",
    "        \n",
    "        if not isProbability(β_optimal_mixed):\n",
    "            print(\"WARNING: not probability:\", β_optimal_mixed)\n",
    "        if display_betas:\n",
    "            print(\"\\tbeta (mixed):\", β_optimal_diagonal)        \n",
    "        \n",
    "        if trial == num_trials-1:\n",
    "            t5 = time.time()\n",
    "            \n",
    "        var = ham.pauli_rep.variance_local(energy, state, β_optimal_mixed)\n",
    "        \n",
    "        if trial == num_trials-1:\n",
    "            t6 = time.time()\n",
    "        \n",
    "        print(\"Mixed variance:\", var)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"Timings:\")\n",
    "    print(\"ground state: {} seconds\".format(int(t1-t0)))\n",
    "    print(\"optimal diagonal beta: {} seconds\".format(int(t2-t1)))\n",
    "    print(\"calculate variance: {} seconds\".format(int(t3-t2)))\n",
    "    print(\"optimal mixed beta (last trial): {} seconds\".format(int(t5-t4)))\n",
    "    print(\"calculate variance (last trial): {} seconds\".format(int(t6-t5)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal variance: 1.8555830498849535\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 1.8546562200043255\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 1.854656258586077\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 1.854656259066888\n",
      "\n",
      "\n",
      "Timings:\n",
      "ground state: 0 seconds\n",
      "optimal diagonal beta: 0 seconds\n",
      "calculate variance: 0 seconds\n",
      "optimal mixed beta (last trial): 0 seconds\n",
      "calculate variance (last trial): 0 seconds\n"
     ]
    }
   ],
   "source": [
    "both_variances_lagrange(h2_jw_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal variance: 17.74194781113713\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 17.457187282027824\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 17.45719122547085\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 17.457191457394067\n",
      "\n",
      "\n",
      "Timings:\n",
      "ground state: 0 seconds\n",
      "optimal diagonal beta: 0 seconds\n",
      "calculate variance: 0 seconds\n",
      "optimal mixed beta (last trial): 0 seconds\n",
      "calculate variance (last trial): 0 seconds\n"
     ]
    }
   ],
   "source": [
    "both_variances_lagrange(h2_jw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal variance: 14.792751908499572\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 14.877408442706624\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 14.87755639740817\n",
      "\n",
      "\n",
      "Trial: \n",
      "Mixed variance: 14.877562484247587\n",
      "\n",
      "\n",
      "Timings:\n",
      "ground state: 0 seconds\n",
      "optimal diagonal beta: 1 seconds\n",
      "calculate variance: 20 seconds\n",
      "optimal mixed beta (last trial): 9 seconds\n",
      "calculate variance (last trial): 22 seconds\n"
     ]
    }
   ],
   "source": [
    "both_variances_lagrange(lih_jw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both_variances_lagrange(h2o_jw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
